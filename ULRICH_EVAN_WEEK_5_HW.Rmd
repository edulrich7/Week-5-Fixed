---
title: "730 Homework - Loans"
author: "Evan Ulrich"
date: "February 15, 2026"
output: html_document
---

## Assignment Context

ABC Bank is developing a system to help determine which loan applications should be denied or adjusted based on predicted repayment risk. The overall goal is to improve repayment rates while using historical loan data, credit reporting, and background information to guide decision making.

---

## Part 1 – Sources of Bias

### 1. Potential sources of bias in the underlying data

There are several ways bias could already exist in the data before a model is even created. One major source is historical decision bias. If past lending decisions were uneven or influenced by subjective factors, those patterns will show up in the training data and the model may learn them. Another source is measurement bias from credit reports or background checks, since not everyone has the same access to credit history and errors or missing information can impact some groups more than others. Selection bias is also possible because the dataset mainly reflects applicants who were approved for loans, which limits visibility into outcomes for denied applicants. Finally, some variables may act as proxies for protected characteristics even if those characteristics are not directly included.

### 2. How biases might be introduced in the data science pipeline

Bias can be introduced at multiple stages of the data science process. It can start with how the problem is framed if success is defined only as maximizing repayment rates without considering fairness. Feature selection can unintentionally include variables that reflect socioeconomic differences rather than true repayment risk. Model evaluation can also introduce bias if performance is measured only by overall accuracy instead of looking at group-level outcomes. Another important area is threshold selection, since changing the cutoff for approval or denial directly affects which applicants are impacted.

### 3. Risks to fairness in downstream applications and deployment

The biggest fairness risk is that certain groups could be denied loans or assigned higher interest rates more often, even when they would have successfully repaid. This creates unequal access to credit and can reinforce existing financial gaps. There is also the risk of feedback loops where denied applicants never get the opportunity to demonstrate repayment success, causing the model to continue reinforcing the same patterns over time.

---

## Part 2 – Bias Metrics

### 1. False positive

In this problem, a false positive occurs when the model predicts that a borrower will not repay their loan on time even though they actually would have. The harm is significant because the person may be denied access to credit or face higher costs unnecessarily. From the bank’s perspective, this also means losing a potentially reliable customer and revenue opportunity.

### 2. False negative

A false negative occurs when the model predicts someone will repay their loan on time but they end up not doing so. This creates financial risk for the bank through defaults, higher collection costs, and potential losses. It can also harm borrowers if they take on debt they are not prepared to manage.

### 3. Metric to focus on for equity

The metric I would focus on for equity in this case is the false positive rate (FPR) across groups. Since the main action of the model is denying loans or increasing penalties, the largest fairness concern is incorrectly labeling applicants as high risk when they would have repaid successfully. Monitoring and reducing differences in false positive rates helps protect equitable access to credit while still allowing the bank to manage overall risk.

---

## Final Reflection

Overall, this type of model can be useful for improving decision making, but only if fairness is considered alongside performance. Bias can enter through the data, the modeling process, and deployment decisions, so evaluating outcomes beyond simple accuracy is critical for responsible use.
